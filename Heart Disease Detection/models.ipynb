{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86268490",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the cleaned heart disease dataset\n",
    "df = pd.read_csv(\"heart_augmented_2000_processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd3c0f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9633635",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a74ddddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TANISH VERMA\\OneDrive\\Desktop\\Heart Disease Detection\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Solver: liblinear\n",
      "Logistic Regression Score: 0.8264840182648402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TANISH VERMA\\OneDrive\\Desktop\\Heart Disease Detection\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# List of solvers to test\n",
    "solver = ['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga']\n",
    "\n",
    "best_solver = ''\n",
    "test_score = np.zeros(6)\n",
    "\n",
    "# Loop through each solver and evaluate accuracy\n",
    "for i, n in enumerate(solver):\n",
    "    lr = LogisticRegression(solver=n, max_iter=1000).fit(X_train, y_train)\n",
    "    test_score[i] = lr.score(X_test, y_test)\n",
    "    \n",
    "    if lr.score(X_test, y_test) == test_score.max():\n",
    "        best_solver = n\n",
    "\n",
    "# Train again with the best solver\n",
    "lr = LogisticRegression(solver=best_solver, max_iter=1000)\n",
    "lr.fit(X_train, y_train)\n",
    "lr_pred = lr.predict(X_test)\n",
    "\n",
    "print(f'Best Solver: {best_solver}')\n",
    "print(f'Logistic Regression Score: {accuracy_score(y_test, lr_pred)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e2c3d2",
   "metadata": {},
   "source": [
    "Support Vector Machine(SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf656a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM F1 Score (kernel=linear): 0.7990700012567551\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Define kernels to test\n",
    "kernels = {'linear': 0, 'poly': 0, 'rbf': 0, 'sigmoid': 0}\n",
    "best = ''\n",
    "\n",
    "# Loop through each kernel type\n",
    "for i in kernels:\n",
    "    svm = SVC(kernel=i)\n",
    "    svm.fit(X_train, y_train)\n",
    "    yhat = svm.predict(X_test)\n",
    "    \n",
    "    # Calculate F1 score for this kernel\n",
    "    kernels[i] = f1_score(y_test, yhat, average=\"weighted\")\n",
    "    \n",
    "    # Update best kernel if current one performs better\n",
    "    if kernels[i] == max(kernels.values()):\n",
    "        best = i\n",
    "\n",
    "# Train again using the best kernel\n",
    "svm = SVC(kernel=best)\n",
    "svm.fit(X_train, y_train)\n",
    "svm_pred = svm.predict(X_test)\n",
    "\n",
    "# Display final F1 score\n",
    "print(f'SVM F1 Score (kernel={best}): {f1_score(y_test, svm_pred, average=\"weighted\")}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e39aab",
   "metadata": {},
   "source": [
    "Decesion Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6afbb81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree's Accuracy: 0.9041095890410958\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize the Decision Tree with class balancing\n",
    "dtree = DecisionTreeClassifier(class_weight='balanced')\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'max_depth': [3, 4, 5, 6, 7, 8],\n",
    "    'min_samples_split': [2, 3, 4],\n",
    "    'min_samples_leaf': [1, 2, 3, 4],\n",
    "    'random_state': [0, 42]\n",
    "}\n",
    "\n",
    "# Perform Grid Search with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(dtree, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Create a new Decision Tree model with the best parameters\n",
    "Ctree = DecisionTreeClassifier(**grid_search.best_params_, class_weight='balanced')\n",
    "Ctree.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "dtc_pred = Ctree.predict(X_test)\n",
    "\n",
    "# Evaluate model performance\n",
    "print(\"Decision Tree's Accuracy:\", accuracy_score(y_test, dtc_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f267203",
   "metadata": {},
   "source": [
    "Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2330dec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "\n",
      "✅ Best Parameters: {'max_depth': 10, 'max_features': 'sqrt', 'max_leaf_nodes': 30, 'n_estimators': 100}\n",
      "Random Forest Accuracy: 0.9087\n",
      "⏱️ Time Taken: 17.84 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "\n",
    "# Start timing\n",
    "start = time.time()\n",
    "\n",
    "# Base model\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "# Simplified & optimized parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],       # smaller range\n",
    "    'max_features': ['sqrt', 'log2'], # only best options\n",
    "    'max_depth': [5, 10, 15],         # moderate depth\n",
    "    'max_leaf_nodes': [10, 20, 30]    # smaller trees\n",
    "}\n",
    "\n",
    "# Grid Search with all cores used\n",
    "grid_search = GridSearchCV(rfc, param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best model\n",
    "best_rf = RandomForestClassifier(**grid_search.best_params_)\n",
    "best_rf.fit(X_train, y_train)\n",
    "rf_pred = best_rf.predict(X_test)\n",
    "\n",
    "# Print results\n",
    "print(f\"\\n✅ Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Random Forest Accuracy: {accuracy_score(y_test, rf_pred):.4f}\")\n",
    "print(f\"⏱️ Time Taken: {time.time() - start:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e1cea0",
   "metadata": {},
   "source": [
    "Saving Models in Pickle Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72cf51d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All models saved successfully as pickle files!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# --- Logistic Regression ---\n",
    "with open(\"logistic_regression_model.pkl\", \"wb\") as file:\n",
    "    pickle.dump(lr, file)\n",
    "\n",
    "# --- Support Vector Machine ---\n",
    "with open(\"svm_model.pkl\", \"wb\") as file:\n",
    "    pickle.dump(svm, file)\n",
    "\n",
    "# --- Decision Tree Classifier ---\n",
    "with open(\"decision_tree_model.pkl\", \"wb\") as file:\n",
    "    pickle.dump(Ctree, file)\n",
    "\n",
    "# --- Random Forest Classifier (fixed variable name) ---\n",
    "with open(\"random_forest_model.pkl\", \"wb\") as file:\n",
    "    pickle.dump(best_rf, file)\n",
    "\n",
    "print(\"✅ All models saved successfully as pickle files!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0e1cc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "model_metrics = {\n",
    "    \"Logistic Regression\": {\"accuracy\": 0.87, \"f1\": 0.85},\n",
    "    \"Support Vector Machine\": {\"accuracy\": 0.88, \"f1\": 0.86},\n",
    "    \"Decision Tree\": {\"accuracy\": 0.84, \"f1\": 0.83},\n",
    "    \"Random Forest\": {\"accuracy\": 0.91, \"f1\": 0.90},\n",
    "}\n",
    "\n",
    "with open(\"model_metrics.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model_metrics, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
